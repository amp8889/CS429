{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 20:07:45 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapybot)\n",
      "2024-03-21 20:07:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 37.0.1, Platform Windows-10-10.0.19045-SP0\n",
      "2024-03-21 20:07:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2024-03-21 20:07:45 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2024-03-21 20:07:45 [scrapy.extensions.telnet] INFO: Telnet Password: 0229f998230127f0\n",
      "2024-03-21 20:07:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-03-21 20:07:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-03-21 20:07:45 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-03-21 20:07:45 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-03-21 20:07:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-03-21 20:07:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Attempting to acquire lock 2129617247680 on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Lock 2129617247680 acquired on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Attempting to acquire lock 2129617505632 on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/urls\\62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Lock 2129617505632 acquired on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/urls\\62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Attempting to release lock 2129617505632 on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/urls\\62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Lock 2129617505632 released on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/urls\\62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Attempting to release lock 2129617247680 on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [filelock] DEBUG: Lock 2129617247680 released on c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com> (referer: None)\n",
      "2024-03-21 20:07:45 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.history.com> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/videos> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://military.history.com/> from <GET http://military.history.com/>\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/profile/information> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/schedule> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/topics> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/shows> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/channel/ancient-aliens-fan-favorites> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.mylifetime.com/lrw/schedule> from <GET https://www.mylifetime.com/schedule/lrw>\n",
      "2024-03-21 20:07:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.historyenespanol.com/> from <GET http://www.historyenespanol.com/>\n",
      "2024-03-21 20:07:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.aenetworks.com/divisions/ad-sales> from <GET https://www.aenetworks.com/advertise>\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/live> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/news> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/this-day-in-history> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:45 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.history.com/military> from <GET https://military.history.com/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/copyright> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.history.com/espanol/schedule> from <GET https://www.historyenespanol.com/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.mylifetime.com/lrw/schedule> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.mylifetime.com/lmn> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.crimeandinvestigationnetwork.com/schedule> from <GET https://www.crimeandinvestigationnetwork.com/>\n",
      "2024-03-21 20:07:46 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.biography.com/. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.biography.com/> (failed 1 times): [<twisted.python.failure.Failure twisted.web.http._MalformedChunkedDataError: Chunk did not end with CRLF>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'TRAILER' state, still expecting more data to get to 'FINISHED' state.>]\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/espanol/schedule> (referer: None)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/apps> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.mylifetime.com/> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.history.com/classroom> from <GET https://www.history.com/shows/classroom>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fyi.tv/> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.history.com/military/schedule> from <GET https://www.history.com/military>\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.aetv.com/ie-upgrade-messaging> from <GET https://www.aetv.com/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/classroom> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.crimeandinvestigationnetwork.com/schedule> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/military/schedule> (referer: None)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.historyvault.com/?cmpid=HV_O_Site_H_Footer> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aetv.com/ie-upgrade-messaging> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/news/the-stamp-act-riots> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/news/lewis-clark-timeline-expedition> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/accessibility-support> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/news/beware-the-ides-of-march-but-why> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/news/selma-bloody-sunday-attack-civil-rights-movement> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/adchoices> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/topics/natural-disasters-and-environment/vernal-spring-equinox> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.historystore.com/?utm_source=history&utm_medium=footer&utm_campaign=shop_link> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/news/equal-pay-act-esther-peterson> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/shows/pawn-stars-do-america> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/shows/the-unxplained-special-presentation> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/guidelines> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/shows/the-secret-of-skinwalker-ranch> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.vicetv.com/en_us/> from <GET https://viceland.com/en_us/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.historyvault.com/?cmpid=HV_O_Site_H_HP_Promo_Tile> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/history-remade-with-sabrina> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/ancient-recipes-with-sohla> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/revelation-the-end-of-days> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.vicetv.com/en_us> from <GET https://www.vicetv.com/en_us/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/ancient-workouts-with-omar> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/the-unxplained> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/careers> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.vicetv.com/en_us> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/swamp-people> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/historys-greatest-mysteries> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/pawn-stars> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://support.history.com/hc> from <GET https://support.history.com/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/alone> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/ancient-aliens> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/abraham-lincoln> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/the-secret-of-skinwalker-ranch> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/washington> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/privacy> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/terms> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/cookie-notice> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/lost-u-boats-of-wwii> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/forged-in-fire> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://instagram.com/history> from <GET http://instagram.com/history>\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://instagram.com/history/> from <GET https://instagram.com/history>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/mountain-men> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.aenetworks.com/divisions/ad-sales> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/the-curse-of-oak-island/season-11/episode-18> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/topics/womens-history> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/historys-greatest-mysteries/season-5/episode-6> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/the-food-that-built-america/season-5/episode-4> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.instagram.com/history/> from <GET https://instagram.com/history/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/lost-u-boats-of-wwii/season-1/episode-3> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/emails/sign-up> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://support.history.com/hc/en-us> from <GET https://support.history.com/hc>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/the-curse-of-oak-island> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/swamp-people/season-15/episode-11> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/ancient-aliens/season-20/episode-10> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.historyvault.com/?cmpid=HV_O_Site_H_Menu> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.history.com/podcasts> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://support.history.com/hc/en-us> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.biography.com/. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.biography.com/> (failed 2 times): [<twisted.python.failure.Failure twisted.web.http._MalformedChunkedDataError: Chunk did not end with CRLF>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'TRAILER' state, still expecting more data to get to 'FINISHED' state.>]\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/american-pickers> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET https://m.facebook.com/History/?_rdr> from <GET https://www.facebook.com/History/>\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (303) to <GET https://www.youtube.com/user/historychannel?sub_confirmation=1> from <GET https://www.youtube.com/subscription_center?add_user=historychannel>\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://www.instagram.com/accounts/login/?next=https%3A%2F%2Fwww.instagram.com%2Fhistory%2F> from <GET https://www.instagram.com/history/>\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://play.history.com/shows/pawn-stars-do-america> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://support.history.com/hc/en-us>: HTTP status code is not handled or not allowed\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.tiktok.com/@history> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.engine] DEBUG: Crawled (400) <GET https://twitter.com/history> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:46 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.biography.com/. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.biography.com/> (failed 3 times): [<twisted.python.failure.Failure twisted.web.http._MalformedChunkedDataError: Chunk did not end with CRLF>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'TRAILER' state, still expecting more data to get to 'FINISHED' state.>]\n",
      "2024-03-21 20:07:46 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://m.facebook.com/login.php?next=https%3A%2F%2Fm.facebook.com%2FHistory%2F&refsrc=deprecated&_rdr> from <GET https://m.facebook.com/History/?_rdr>\n",
      "2024-03-21 20:07:47 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <400 https://twitter.com/history>: HTTP status code is not handled or not allowed\n",
      "2024-03-21 20:07:47 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.biography.com/>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Aidan\\anaconda3\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 49, in process_request\n",
      "    return (yield download_func(request=request, spider=spider))\n",
      "twisted.web._newclient.ResponseFailed: [<twisted.python.failure.Failure twisted.web.http._MalformedChunkedDataError: Chunk did not end with CRLF>, <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'TRAILER' state, still expecting more data to get to 'FINISHED' state.>]\n",
      "2024-03-21 20:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://m.facebook.com/login.php?next=https%3A%2F%2Fm.facebook.com%2FHistory%2F&refsrc=deprecated&_rdr> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.instagram.com/accounts/login/?next=https%3A%2F%2Fwww.instagram.com%2Fhistory%2F> (referer: None)\n",
      "2024-03-21 20:07:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.youtube.com/user/historychannel?sub_confirmation=1> (referer: https://www.history.com)\n",
      "2024-03-21 20:07:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2024-03-21 20:07:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 3,\n",
      " 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,\n",
      " 'downloader/request_bytes': 34056,\n",
      " 'downloader/request_count': 102,\n",
      " 'downloader/request_method_count/GET': 102,\n",
      " 'downloader/response_bytes': 10268589,\n",
      " 'downloader/response_count': 99,\n",
      " 'downloader/response_status_count/200': 77,\n",
      " 'downloader/response_status_count/301': 15,\n",
      " 'downloader/response_status_count/302': 4,\n",
      " 'downloader/response_status_count/303': 1,\n",
      " 'downloader/response_status_count/400': 1,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'dupefilter/filtered': 49,\n",
      " 'elapsed_time_seconds': 1.761291,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2024, 3, 22, 1, 7, 47, 394668),\n",
      " 'httpcompression/response_bytes': 35644599,\n",
      " 'httpcompression/response_count': 79,\n",
      " 'httperror/response_ignored_count': 2,\n",
      " 'httperror/response_ignored_status_count/400': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 111,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 12,\n",
      " 'log_count/WARNING': 3,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 78,\n",
      " 'retry/count': 2,\n",
      " 'retry/max_reached': 1,\n",
      " 'retry/reason_count/twisted.web._newclient.ResponseFailed': 2,\n",
      " 'scheduler/dequeued': 102,\n",
      " 'scheduler/dequeued/memory': 102,\n",
      " 'scheduler/enqueued': 102,\n",
      " 'scheduler/enqueued/memory': 102,\n",
      " 'start_time': datetime.datetime(2024, 3, 22, 1, 7, 45, 633377)}\n",
      "2024-03-21 20:07:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class HtmlDocumentCrawler(scrapy.Spider):\n",
    "    name = 'html_document_crawler'\n",
    "\n",
    "    def __init__(self, seed_url=None, max_pages=None, max_depth=None, *args, **kwargs):\n",
    "        super(HtmlDocumentCrawler, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [seed_url] if seed_url else []\n",
    "        self.max_pages = int(max_pages) if max_pages else None\n",
    "        self.max_depth = int(max_depth) if max_depth else None\n",
    "        self.pages_crawled = 0\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, self.parse, meta={'depth': 0})\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract HTML content\n",
    "        html_content = response.body\n",
    "\n",
    "        # Save the HTML content to a file\n",
    "        filename = f'page_{response.url.split(\"/\")[-2]}.html'  # Create a filename from the URL\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "        self.pages_crawled += 1\n",
    "\n",
    "        # Check if the maximum number of pages has been reached\n",
    "        if self.max_pages is not None and self.pages_crawled >= self.max_pages:\n",
    "            return\n",
    "\n",
    "        # Continue crawling if maximum depth criteria is not met\n",
    "        if self.max_depth is None or response.meta['depth'] < self.max_depth:\n",
    "            for link in response.css('a::attr(href)').extract():\n",
    "                yield response.follow(link, callback=self.parse, meta={'depth': response.meta['depth'] + 1})\n",
    "\n",
    "\n",
    "# Run the crawler within a Jupyter Notebook cell\n",
    "seed_url = \"https://www.history.com\"  # Set your seed URL here\n",
    "max_pages = 10  # Set the maximum number of pages to crawl\n",
    "max_depth = 1  # Set the maximum depth of the crawling\n",
    "\n",
    "# Create a CrawlerProcess\n",
    "process = CrawlerProcess(settings={\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "# Start the crawler with HtmlDocumentCrawler\n",
    "process.crawl(HtmlDocumentCrawler, seed_url=seed_url, max_pages=max_pages, max_depth=max_depth)\n",
    "process.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
